---
layout: post
title: llm-mem
description: my learning note of the memory requirements of llm
tags: llm, memory, requirements
---

Note of llm training memory requirements.


### Who lives inside of the memory during the llm training?
1. Model State
    - model parameters
    - gradients
    - optimizer states

2. Residual State
    - activations
    - temporary buffers
    - unusable memory fragments

### Before deepspeed how to scale?

| Feature                     | Data Parallelism (DP)                                 | Model Parallelism (MP)                                | Pipeline Parallelism (PP)                                |
|-----------------------------|-------------------------------------------------------|------------------------------------------------------|----------------------------------------------------------|
| **Definition**              | Distributing data across multiple devices             | Distributing different parts of the model across multiple devices    | Dividing the model into stages and running them in sequence on different devices |
| **Use Case**                | Large datasets                                        | Very large models that cannot fit into a single device's memory                                          | Extremely large models that benefit from being split into sequential stages |
| **How it works**            | Each device gets a different subset of the data and trains a copy of the model | Different layers or parts of the model are placed on different devices | The model is split into stages, with each stage processed on different devices in sequence |
| **Communication Overhead**  | Moderate, synchronization needed after each mini-batch| High, requires communication between devices for forward and backward passes          | Moderate, data passed between stages after each mini-batch but less frequent than MP |
| **Memory Usage**            | Efficient, data is divided among devices              | Efficient, each device only needs to store part of the model | Moderate, memory divided among stages but requires storing intermediate outputs |
| **Scalability**             | Good for many devices                                 | Limited by how well the model can be partitioned into smaller segments          | Good, but dependent on the balance of computation and communication across stages |
| **Implementation Complexity**| Simple, widely supported by frameworks               | Complex, requires careful model partitioning and synchronization         | Moderate, requires managing inter-stage communication and load balancing |
| **Latency**                 | Low, as all devices process data in parallel          | High, as devices need to communicate frequently          | Moderate, can be affected by the time taken to pass data between stages |
| **Fault Tolerance**         | High, losing a device only affects that subset of data| Low, losing a device means losing part of the model          | Moderate, losing a device affects that particular stage of the pipeline |
| **Examples**                | ResNet, VGG                                           | Transformers, BERT with large layers                             | GPT-3, T5, and other large transformer models                             |




