---
layout: post
title: DPO
description: learning how to use DPO to do the alignment
tags: LLM, DPO
---

### RL Bradley-Terry model

RL-based objective used by existing methods can be optimized exactly with simple 
binary cross-entropy objective. 

At high level. the objective of RLHF is maximize the reward without drifting too much 
from the original model (that's why we need baseline model)

Sampling from the LM polciy is k-step bootstrapping is the root cause of why RLHF is not stable, and so expensive.

DPO uses a change of variables to define the preference loss as a function of the policy.

Bandit and RL settings. Contextual bandit learning using preferences or rankings of actions rather than rewards.

### How does DPO work

RL approach: the reward model needed.
DPO: change-of-variables. mapping from reward functions to optimal policies. transform a loss function 
over reward functions into a loss function over policies. 


