---
layout: post
title: Why DPO with trl can't use mutiple gpus
description: learning how to use DPO to do the alignment
tags: LLM, DPO, Onion Learning
---
This blog is my learning jouney of running DPO training job for phi-2 model

### The DPO trainer can support 3 type of training settings:

- Input of the base model and reference model 
- Input the base model with a peft config of qlora/lora
- Input the base model with two set of peft config of qlora/lora

### Cluster

2 A10 GPUs to run the job. Each of the GPU has 24 GB memeory. 

### trl's DPO trainer:

I am trying to use the trl's DPO trainer to do the training.

The DPO trainer allow 3 ways of initilization. When does the trainer put the models into the device? When does the trainer run the training step will have a huge impact for the performance of the training process. However since the api call is hiding all those information from the user. We have to be careful to set up the job.


Under the hood, what does the DPO trainer do is:

It inherits the trainer class from the transformers package, which only has one model inside. so the reference model is treated as an additional one. So unless you are using the ZeRO-3 from deepspeed. The reference model will be loaded to the same device with the policy model. 


1. pass two model object (one policy model, one reference model)

2. pass one model with one adapter

3. pass one model with two adapter



