---
layout: post
title: Why DPO with trl can't use mutiple gpus
description: learning how to use DPO to do the alignment
tags: LLM, DPO, Onion Learning
---
This blog is my learning jouney of running DPO training job for phi-2 model

### The DPO trainer can support 3 type of training settings:

- Input of the base model and reference model 
- Input the base model with a peft config of qlora/lora
- Input the base model with two set of peft config of qlora/lora

### Cluster

2 A10 GPUs to run the job. Each of the GPU has 24 GB memeory. 

### trl's DPO trainer:

I am trying to use the trl's DPO trainer to do the training.

The DPO trainer allow 3 ways of initilization. When does the trainer put the models into the device? When does the trainer run the training step will have a huge impact for the performance of the training process. However since the api call is hiding all those information from the user. We have to be careful to set up the job.


Under the hood, what does the DPO trainer do is:

It inherits the trainer class from the transformers package, which only has one model inside. so the reference model is treated as an additional one. So unless you are using the ZeRO-3 from deepspeed. The reference model will be loaded to the same device with the policy model. It will use the reference model to generate the reference_log_probs, append the value to the dataloader. Then call the transformers's trainer to do the training. 


For Phi-2, with 2.7 billion parameters, the memory requirement for loading the model is approximately 2.7 x 4 = 10.8 GB. Itâ€™s important to note that this is solely for loading the model; during training, the memory usage expands, often doubling the initial requirement. And with Adam optimizer, it will quadruple it.


1. pass two model object (one policy model, one reference model)
Can't do for our setting, it gave OOM.

2. pass one model with one adapter
Can do the work with quantization of the model. Can't run in mutiple gpus with deepspeed.

3. pass one model with two adapters
Can do the work with quantization of the model. Can't run in mutiple gpus with deepspeed. 

### Resons of why the deepspeed doesn't work
DeepSpeed is not compatible with BitsandBytes. You can either use Trainer + PEFT + DeepSpeed ZeRO3 or Trainer + PEFT + BitsandBytes/int4/int8. 

I have difficuty to run the deepspeed ZeRO3 with the DPO trainer. 


### Known issues

- After each step cuda memory didn't release
We can overwrite the training_step in the dpo_trainer 
```
    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:
        loss_step = super().training_step(model, inputs)
        torch.cuda.empty_cache()
        gc.collect()
        return loss_step
```
Have the same error with this issue https://github.com/huggingface/trl/issues/1377

- The following code that only loads data in the main process gives error 

```
# Compute that only on the main process for faster data processing.
        # see: https://github.com/huggingface/trl/pull/1255
        with PartialState().local_main_process_first():
            # tokenize the dataset
            train_dataset = train_dataset.map(self.tokenize_row, num_proc=self.dataset_num_proc)
            if eval_dataset is not None:
                eval_dataset = eval_dataset.map(self.tokenize_row, num_proc=self.dataset_num_proc)
```

Trying to investigate why the deepspeed can't run.





