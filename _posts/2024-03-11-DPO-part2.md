---
layout: post
title: Why DPO with trl can't use mutiple gpus
description: learning how to use DPO to do the alignment
tags: LLM, DPO, Onion Learning
---

This blog is trying to figure out why the trl's DPO trainer can't run on mutiple GPUs. And find a temperal solution to address my need

### The issue when doing DPO with trl

I am trying to do the alignment with DPO training with adapter. when I spin up two A10 gpus to train the model. I got the following error message. 

```python
RuntimeError                              Traceback (most recent call last)
Cell In[8], line 14
      1 trainer = DPOTrainer(
      2     model,
      3     ref_model = None,
   (...)
     12     #dataset_num_proc=4,
     13 )
---> 14 trainer.train()
     15 #cProfile.run('trainer.train()')

File ~/.conda/envs/llm_judge/lib/python3.9/site-packages/transformers/trainer.py:1624, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1622         hf_hub_utils.enable_progress_bars()
   1623 else:
-> 1624     return inner_training_loop(
   1625         args=args,
   1626         resume_from_checkpoint=resume_from_checkpoint,
   1627         trial=trial,
   1628         ignore_keys_for_eval=ignore_keys_for_eval,
   1629     )

File ~/.conda/envs/llm_judge/lib/python3.9/site-packages/transformers/trainer.py:1971, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1969     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)
   1970 else:
-> 1971     tr_loss += tr_loss_step
   1973 self.current_flos += float(self.floating_point_ops(inputs))
   1975 is_last_step_and_steps_less_than_grad_acc = (
   1976     steps_in_epoch <= args.gradient_accumulation_steps and (step + 1) == steps_in_epoch
   1977 )

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!
```

It seems during the training, when update the `tr_loss`, the `tr_loss_step` tensor is on different devices.

