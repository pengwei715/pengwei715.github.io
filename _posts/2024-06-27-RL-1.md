---
layout: post
title: RL-Course-1
description: my learning note of the RL cource
tags: RL
---

Note one of RL course from Alberta University.
k-armed bandit problem
action-value methods


### Roadmap

1. Mutlti-armed bandit problem (MAB)
2. Markov Decision Process (MDP)
3. Dynamic programming


### K-armed bandit problem

Agent needs to choose from k actions, each action will have a unknown reward.
Once the action is performed, the agent see the reward.
- reward
- time steps
- values
The values of the action is the expected reward of taking that action.
The goal of the agent is to maximize the expected reward.
argmax_a $$Q(a) = a*$$. which means the argument which maximizes the q* function.


### Action-value methods

$$ q*(a) = E[R_t | A_t = a] $$ 

This number is unkown, we need to estimate it.

- Sample-average method
sum of the rewards when a is taken prior to t / number of times a is taken prior to t

$$Q_t(a) = \frac{sum \: of \: rewards \: when \: a \: is \: taken \: prior \: to \: t}{\: number \: of \: times \: a \: is \: taken \: prior \: to \: t}$$

- Greedy action selection
Choose the action with the highest estimated value.


- exploration-exploitation dilemma


### Sequential decision making with evaluative feedback

The agent needs to make a sequence of decisions, and need to learn from the error and gain.

The feedback needs to be quantitative.

### Learning action

what if the doctors know what's the long term effect of the action. then choosing action will be easy.


### Estimating action values incrementally

incremental update rule for estimating action values

$$

Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i
 = \frac{1}{n} (R_n + \sum_{i=1}^{n-1} R_i) 
 = \frac{1}{n} (R_n + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1} R_i) 
 = \frac{1}{n} (R_n + (n-1)Q_{n})
 = \frac{1}{n}(R_n + nQ_{n} - Q_{n})
$$


$$ NewEstimate = OldEstimate + StepSize(Target - OldEstimate) $$

The error of the estimate is the difference between the new target and OldEstimate

The new reward is our target

The StepSize can be a function of $n$ that between 0 and 1. 

general update rule can be used to solve a non-stationary bantdit problem.

### Non-stationary bandit problem

The distribution of reward changes over time. 
If the step size is constant, the most recent rewards will have the most influence on the estimate than the old rewards. 

Decaying the past rewards

### Exploration and exploitation trade-off
When to explore and when to exploit?

exploration: improve knowledge for long-term benefit
exploitation: maximize reward in the short term

### Epsilon-greedy methods

With probability $$\epsilon$$, select a random action. With probability $$1-\epsilon$$, select the greedy action.


$$
A_t = \begin{cases}
argmax_a Q_t(a) & \text{with probability 1 - epsilon} \\
random(a) & \text{with probability epsilon}
\end{cases}
$$ 

use mutiple run's result to get the average reward.

### Optimistic initial values

The optimistic initial values can encourage early exploration. ( the agent will not be stuck in the suboptimal action at
the very beginning)

The limitation of the optimistic initial values
- only drive early exploration
- not well-suited for non-stantinary problems
- how to set the optimistic initial value is unknown.



### Upper-Confidence-Bound Action Selection
UCB is using the uncertainty of the action value to balance the exploration and exploitation.

if we are uncertain about the action value, we should use the upper bound.
$$
A_t = argmax_a [Q_t(a) + c \sqrt{\frac{ln(t)}{N_t(a)}}]
$$
The first term is the exploitation part. the second term is the exploration part.



### Greedy Bandit Algorithms
The update step is the most important part of the algorithm.
Clearly, we need env and agent in the RL setting.

```python

class GreedyAgent(main_agent.Agent):
    def agent_step(self, reward, observation=None):
        """
        Takes one step for the agent. It takes in a reward and observation and 
        returns the action the agent chooses at that time step.
        
        Arguments:
        reward -- float, the reward the agent recieved from the environment after taking the last action.
        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it
                              until future lessons
        Returns:
        current_action -- int, the action chosen by the agent at the current time step.
        """
        ### Useful Class Variables ###
        # self.q_values : An array with what the agent believes each of the values of the arm are.
        # self.arm_count : An array with a count of the number of times each arm has been pulled.
        # self.last_action : The action that the agent took on the previous time step
        #######################
        
        # Update Q values Hint: Look at the algorithm in section 2.4 of the textbook.
        # increment the counter in self.arm_count for the action from the previous time step
        # update the step size using self.arm_count
        # update self.q_values for the action from the previous time step


        self.arm_count[self.last_action] += 1
        alpha = 1 / self.arm_count[self.last_action] 
        self.q_values[self.last_action] += alpha * (reward - self.q_values[self.last_action]) 
        current_action = argmax(self.q_values)    
        self.last_action = current_action
        return current_action
```


### Epsilon-Greedy Bandit Algorithm

```python
class EpsilonGreedyAgent(main_agent.Agent):
    def agent_step(self, reward, observation):
        """
        Takes one step for the agent. It takes in a reward and observation and 
        returns the action the agent chooses at that time step.
        
        Arguments:
        reward -- float, the reward the agent recieved from the environment after taking the last action.
        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it
                              until future lessons
        Returns:
        current_action -- int, the action chosen by the agent at the current time step.
        """
        
        ### Useful Class Variables ###
        # self.q_values : An array with what the agent believes each of the values of the arm are.
        # self.arm_count : An array with a count of the number of times each arm has been pulled.
        # self.last_action : The action that the agent took on the previous time step
        # self.epsilon : The probability an epsilon greedy agent will explore (ranges between 0 and 1)
        #######################
        
        # Update Q values - this should be the same update as your greedy agent above
 
        # Choose action using epsilon greedy
        # Randomly choose a number between 0 and 1 and see if it's less than self.epsilon
        # (hint: look at np.random.random()). If it is, set current_action to a random action.
        # otherwise choose current_action greedily as you did above.
  
        self.arm_count[self.last_action] += 1
        alpha = 1 / self.arm_count[self.last_action] 
        self.q_values[self.last_action] += alpha * (reward - self.q_values[self.last_action])
        random_float = np.random.random()
        if random_float < self.epsilon:
            current_action = np.random.randint(len(self.arm_count))
        else:
            current_action = argmax(self.q_values)
        
        self.last_action = current_action
        
        return current_action
```

### Markov Decision Process

The bandit reward is only dependent on the most recent action. In the MDP, the reward is dependent on the current state and action.

#### Formolize the interaction between the agent and the environment.

- agent: the learner and decision maker
- environment: the thing that the agent interacts with.
- action: the decision that the agent makes
- state: the current situation

At each time, agent receives a state from the environment, then agent selects an action, then the environment returns a
reward and the next state. 

The transition function tells us the probability of the next state and reward given the current state and action. The
state, action, and reward are finite.

Markov property: the future state is only dependent on the current state and action.

MDPs provide a framework for modeling sequential decision making.

The dynamics of an MDP are defined by a probalilty distribution.


#### Example: Recycling Robot

- battery state: low and high

- action: based on different battery state, the robot can choose from different actions.
if the state is high, the robot can do search or wait.
if the state is low, the robot can do search or recharge or wait.

- The transition dynamics are the critical part of the MDP. and even for RL.

- Search with high battery state may affect the battery state. 
it can remain in high or change to low. there is a probability envolved.
The probability of staying in the high state is $$/alpha$$, and the probability of changing to the low state is $$1-\alpha$$.
Both will give the robot the reward of search action.
if it decides to wait, the robot will stay in the same state and get a reward of waiting.




- States can be low-level sensory readings or high-level abstractions.

- Actions can be low-level motor commands or high-level goals.

- Time steps can be milliseconds or years.

- These make the MDP framework very flexible.

### Goal of RL

#### How the reward is related to the goal of the agent.

- the agent should have long-term goals.

#### identify the episodic tasks.







