---
layout: post
title: RL-Course-1
description: my learning note of the RL cource
tags: RL
---

Note one of RL course from Alberta University.
k-armed bandit problem
action-value methods


### Roadmap

1. Mutlti-armed bandit problem (MAB)
2. Markov Decision Process (MDP)
3. Dynamic programming


### K-armed bandit problem

Agent needs to choose from k actions, each action will have a unknown reward.
Once the action is performed, the agent see the reward.
- reward
- time steps
- values
The values of the action is the expected reward of taking that action.
The goal of the agent is to maximize the expected reward.
argmax_a $$Q(a) = a*$$. which means the argument which maximizes the q* function.


### Action-value methods

$$ q*(a) = E[R_t | A_t = a] $$ 

This number is unkown, we need to estimate it.

- Sample-average method
sum of the rewards when a is taken prior to t / number of times a is taken prior to t

$$Q_t(a) = \frac{sum \: of \: rewards \: when \: a \: is \: taken \: prior \: to \: t}{\: number \: of \: times \: a \: is \: taken \: prior \: to \: t}$$

- Greedy action selection
Choose the action with the highest estimated value.


- exploration-exploitation dilemma


### Sequential decision making with evaluative feedback

The agent needs to make a sequence of decisions, and need to learn from the error and gain.

The feedback needs to be quantitative.

### Learning action

what if the doctors know what's the long term effect of the action. then choosing action will be easy.


### Estimating action values incrementally

incremental update rule for estimating action values

$$

Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i
 = \frac{1}{n} (R_n + \sum_{i=1}^{n-1} R_i) 
 = \frac{1}{n} (R_n + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1} R_i) 
 = \frac{1}{n} (R_n + (n-1)Q_{n})
 = \frac{1}{n}(R_n + nQ_{n} - Q_{n})
$$


$$ NewEstimate = OldEstimate + StepSize(Target - OldEstimate) $$

The error of the estimate is the difference between the new target and OldEstimate

The new reward is our target

The StepSize can be a function of $n$ that between 0 and 1. 

general update rule can be used to solve a non-stationary bantdit problem.

### Non-stationary bandit problem

The distribution of reward changes over time. 
If the step size is constant, the most recent rewards will have the most influence on the estimate than the old rewards. 

Decaying the past rewards

### Exploration and exploitation trade-off
When to explore and when to exploit?

exploration: improve knowledge for long-term benefit
exploitation: maximize reward in the short term

### Epsilon-greedy methods

With probability $$\epsilon$$, select a random action. With probability $$1-\epsilon$$, select the greedy action.


$$
A_t = \begin{cases}
argmax_a Q_t(a) & \text{with probability 1 - epsilon} \\
random(a) & \text{with probability epsilon}
\end{cases}
$$ 

use mutiple run's result to get the average reward.

### Optimistic initial values

The optimistic initial values can encourage early exploration. ( the agent will not be stuck in the suboptimal action at
the very beginning)

The limitation of the optimistic initial values
- only drive early exploration
- not well-suited for non-stantinary problems
- how to set the optimistic initial value is unknown.



### Upper-Confidence-Bound Action Selection
UCB is using the uncertainty of the action value to balance the exploration and exploitation.

if we are uncertain about the action value, we should use the upper bound.
$$
A_t = argmax_a [Q_t(a) + c \sqrt{\frac{ln(t)}{N_t(a)}}]
$$
The first term is the exploitation part. the second term is the exploration part.




