---
layout: post
title: RL-Course-1
description: my learning note of the RL cource
tags: RL
---

Note one of RL course from Alberta University.
k-armed bandit problem
action-value methods


### Roadmap

1. Mutlti-armed bandit problem (MAB)
2. Markov Decision Process (MDP)
3. Dynamic programming


### K-armed bandit problem

Agent needs to choose from k actions, each action will have a unknown reward.
Once the action is performed, the agent see the reward.
- reward
- time steps
- values
The values of the action is the expected reward of taking that action.
The goal of the agent is to maximize the expected reward.
argmax_a $$Q(a) = a*$$. which means the argument which maximizes the q* function.


### Action-value methods

$$ q*(a) = E[R_t | A_t = a] $$ 

This number is unkown, we need to estimate it.

- Sample-average method
sum of the rewards when a is taken prior to t / number of times a is taken prior to t

$$Q_t(a) = \frac{sum \: of \: rewards \: when \: a \: is \: taken \: prior \: to \: t}{\: number \: of \: times \: a \: is \: taken \: prior \: to \: t}$$

- Greedy action selection
Choose the action with the highest estimated value.


- exploration-exploitation dilemma


### Sequential decision making with evaluative feedback

The agent needs to make a sequence of decisions, and need to learn from the error and gain.

The feedback needs to be quantitative.

### Learning action

what if the doctors know what's the long term effect of the action. then choosing action will be easy.


### Estimating action values incrementally

incremental update rule for estimating action values

$$Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i$$

general update rule can be used to solve a non-stationary bantdit problem.

