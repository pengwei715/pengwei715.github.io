---
layout: post
title: RL-Course-2
description: my learning note of the RL cource
tags: RL
---

Note two of RL course from Alberta University.

### Monte Carlo

It enables the agent to learn from experience in the form of episodes of experience. It requires no model of the environment, only experience, and it can be used with simulation or real experience.

To use pure DP, the agent needs to know the env's transition probabilities.

Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. 

If we use simulated experience, a model is requried, however, we can only generate sample transitions not the complete
probalility distribution.

Monte carlo methods estimate values by averaging over a large number of random samples.


#### Monte carlo used for estimate value functions from sample interaction


MC prediction, for estimating V
```
input: a policy $\pi$ to be evaluated

Initialize:
- $V(s) \in R$ for all $s \in S$
- Returns(s) $\leftarrow$ an empty list for all $s \in S$

Loop forever (for each episode):
- Generate an episode following $\pi$: $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$
- $G \leftarrow 0$
  loop for each step of episode, $t=T-1, T-2, ..., 0$:
    - $G \leftarrow \gamma G + R_{t+1}$
    - unless $S_t$ appears in $S_0, S_1, ..., S_{t-1}$:
    - Append $G$ to Returns($S_t$)
    - $V(S_t) \leftarrow average(Returns(S_t))$
```

#### Using monte carlo for Prediction

