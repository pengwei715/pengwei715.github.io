---
layout: post
title: RL-Course-2
description: my learning note of the RL cource
tags: RL
---

Note two of RL course from Alberta University.

### Monte Carlo

It enables the agent to learn from experience in the form of episodes of experience. It requires no model of the environment, only experience, and it can be used with simulation or real experience.

To use pure DP, the agent needs to know the env's transition probabilities.

Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. 

If we use simulated experience, a model is requried, however, we can only generate sample transitions not the complete
probalility distribution.

Monte carlo methods estimate values by averaging over a large number of random samples.


#### Monte carlo used for estimate value functions from sample interaction


MC prediction, for estimating V



input: a policy $\pi$ to be evaluated

Initialize:
- $V(s) \in R$ for all $s \in S$
- Returns(s) $\leftarrow$ an empty list for all $s \in S$

Loop forever (for each episode):
- Generate an episode following $\pi$: $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$
- $G \leftarrow 0$
  loop for each step of episode, $t=T-1, T-2, ..., 0$:
    - $G \leftarrow \gamma G + R_{t+1}$
    - unless $S_t$ appears in $S_0, S_1, ..., S_{t-1}$:
    - Append $G$ to Returns($S_t$)
    - $V(S_t) \leftarrow average(Returns(S_t))$



Both first-visit MC and every-visit MC converge to the true value function as the number of visits to each state approaches infinity. By the law of large numbers the sequence of averages of these estimates converges to thier expected values.

each average is iteself an unbiased estimate, and the var of its error as $1/sqrt(N)$, where N is the number of samples.

### Using Monte Carlo for action values

#### Estimate action-value function using MC

Action values are useful for learning a policy.


#### maintaining exploration in MC

1. exploring stats

Epicodes start in every state-action pair.

2. $\epsilon$-greedy strategy


### Monte carlo building generalized policy iteration

Generalized policy iteration (GPI) is a way to combine policy evaluation and improvement.

#### How to use Monte Carlo to implement GPI?

$${pi}_{k+1} = argmax_{a} Q_{\pi_k}(s, a)$$

For evaluation, Monte Carlo Prediction can be used to estimate the value function of the policy.

initialize:
   - {pi}(s) in A for all s in S
   - Q(s, a) in R for all s in S, a in A(s)
   - Returns(s, a) an empty list for all s in S, a in A(s)

loop forever (for each episode):
   - choose S_0 in S, A_0 in A(S_0) randomly such that all pairs have probability > 0
   - generate an episode starting from S_0, A_0, following {pi}
   - G <- 0
   - loop for each step of the episode, t= T-1, T-2, ..., 0:
      - G <- gamma * G + R_{t+1}
      - Append G to Returns(S_t, A_t)
      - Q(S_t, A_t) <- average(Returns(S_t, A_t))
      - {pi}(S_t) <- argmax_{a} Q(S_t, a)
